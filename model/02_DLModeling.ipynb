{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3882516f-d25e-4e67-acf3-e34062f19f1a",
   "metadata": {},
   "source": [
    "# 02 - Deep learning Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fa8148-aeb8-444f-8880-a065f43ab263",
   "metadata": {},
   "source": [
    "In this notebook, the focus shifts to designing the deep learning architecture for the image classification task.\n",
    "\n",
    "- Neural Network Structure: Utilizing both torch.nn and torch.nn.functional, the notebook guides through the process of constructing a neural network tailored for satellite image classification. This involves defining layers, activation functions, and other architectural elements.\n",
    "\n",
    "- Code Modularization: To enhance readability and reusability, the neural network code is encapsulated into a separate Python module named landneuralnetwork.py within the mypackages directory. This modular approach simplifies future imports and usage of the model architecture in subsequent notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee31cfec-81f6-4e66-8231-78b10344e9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04c0a25-3b02-4087-a89a-8a2a4bd0a240",
   "metadata": {},
   "source": [
    "In PyTorch, torch.nn and torch.nn.functional are two core modules used extensively for building neural networks. They provide the building blocks for designing layers and functions in models, but they differ in their level of abstraction and the way they are used.\n",
    "\n",
    "### torch.nn\n",
    "**Module-based:** torch.nn provides a set of classes that can be used to create neural network layers which are encapsulated as objects. Each class in torch.nn is a subclass of nn.Module, which is a base class for all neural network modules in PyTorch.\n",
    "\n",
    "**Stateful:** Layers created using torch.nn classes hold state in the form of weights and biases that can be automatically managed by PyTorch. This includes layers like nn.Linear, nn.Conv2d, and many others that have trainable parameters.\n",
    "\n",
    "**Higher-level abstraction:** torch.nn is suitable for building complex networks with reusable layers and without worrying about the underlying operations. It's designed to provide a high-level API for neural network construction.\n",
    "Examples: nn.Conv2d for convolutional operations, nn.ReLU for ReLU activation, nn.Linear for fully connected layers, etc.\n",
    "\n",
    "### torch.nn.functional\n",
    "**Function-based:** torch.nn.functional contains functions that perform specific operations and can be considered as stateless versions of the corresponding torch.nn modules. These functions directly operate on the input tensors and return output tensors.\n",
    "\n",
    "**Stateless:** Unlike torch.nn modules, torch.nn.functional functions do not have any internal state. This means they do not automatically manage parameters, making them ideal for operations that do not require learning weights, such as activation functions (F.relu), pooling operations (F.max_pool2d), and loss functions (F.cross_entropy).\n",
    "\n",
    "**Lower-level operations:** torch.nn.functional is more flexible and allows for more fine-grained control over the operations since it works directly with tensor operations. It's often used in custom layers or in situations where the behavior slightly differs from the standard layer definitions.\n",
    "\n",
    "**Examples:** F.conv2d for convolutional operations, F.relu for ReLU activation, F.linear for fully connected operations, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fda56dc-46bc-4bb8-89ec-89f202620676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defines a new model class named LandClassifierNet, inheriting from nn.Module\n",
    "class LandClassifierNet(nn.Module):\n",
    "    # Constructor method of the class\n",
    "    def __init__(self):\n",
    "        # Calls the constructor of the parent class (nn.Module)\n",
    "        super(LandClassifierNet, self).__init__()\n",
    "\n",
    "        # Defines the first convolutional layer with 3 input channels, 64 output channels,\n",
    "        # and a kernel size of 3x3. This layer is responsible for capturing low-level features\n",
    "        # such as edges and textures from the input images.\n",
    "        self.conv1 = nn.Conv2d(3, 64, 3, 1)\n",
    "\n",
    "        # Defines the second convolutional layer with 64 input channels, 128 output channels,\n",
    "        # and a kernel size of 3x3. This layer further processes features extracted by the\n",
    "        # previous layer, capturing more complex patterns.\n",
    "        self.conv2 = nn.Conv2d(64, 128, 3, 1)\n",
    "\n",
    "        # Defines the third convolutional layer with 128 input channels, 256 output channels,\n",
    "        # and a kernel size of 3x3. Each subsequent convolutional layer increases the ability\n",
    "        # of the network to represent higher-level features.\n",
    "        self.conv3 = nn.Conv2d(128, 256, 3, 1)\n",
    "\n",
    "        # Defines the first dropout layer with a dropout probability of 0.25.\n",
    "        # Dropout is a regularization technique to prevent overfitting by randomly\n",
    "        # setting a fraction of input units to 0 during training.\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "\n",
    "        # Defines the second dropout layer with a dropout probability of 0.5.\n",
    "        # Higher dropout rates are typically used in layers closer to the output.\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "\n",
    "        # Defines the first fully connected (Dense) layer that maps from 215296 to 2048 neurons.\n",
    "        # The fully connected layers act as classifiers on top of the features extracted by the convolutional layers.\n",
    "        self.fc1 = nn.Linear(215296, 2048)\n",
    "\n",
    "        # Defines the second fully connected layer that maps from 2048 to 512 neurons.\n",
    "        self.fc2 = nn.Linear(2048, 512)\n",
    "\n",
    "        # Defines the third fully connected layer that maps from 512 to 128 neurons.\n",
    "        self.fc3 = nn.Linear(512, 128)\n",
    "\n",
    "        # Defines the fourth fully connected layer that maps from 128 to 10 neurons.\n",
    "        # The output size of 10 corresponds to the number of classes in the classification task.\n",
    "        self.fc4 = nn.Linear(128, 10)\n",
    "\n",
    "    # Defines the forward method for the forward pass\n",
    "    def forward(self, x):\n",
    "        # Applies the convolutional layer abd tge ReLU activation function\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        x = self.conv3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Applies max pooling with a kernel size of 2x2\n",
    "        # Max pooling reduces the spatial dimensions (height, width) of the input volume.\n",
    "        x = F.max_pool2d(x, 2)\n",
    "\n",
    "        # Applies the first dropout layer\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        # Flattens the tensor to prepare it for the fully connected layer\n",
    "        x = torch.flatten(x, 1)\n",
    "\n",
    "        # Applies the first fully connected layer\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Applies the second dropout layer\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        # Applies the second fully connected layer\n",
    "        x = self.fc2(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Applies the third fully connected layer\n",
    "        x = self.fc3(x)\n",
    "        x = F.relu(x)\n",
    "\n",
    "        # Applies the fourth fully connected layer\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        # Returns the log-softmax of the resulting tensor along dimension 1\n",
    "        # Log-softmax is typically used for classification tasks as it provides\n",
    "        # a probability distribution over classes.\n",
    "        return F.log_softmax(x, dim=1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
